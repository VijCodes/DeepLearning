{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f251e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "database_path =  r\"C:\\Users\\formy\\Downloads\\database.sqlite\\database.sqlite\"\n",
    "conn = sqlite3.connect(database_path)\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT * FROM Tweets')\n",
    "data = c.fetchall()\n",
    "columns = ['id', 'airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'airline', 'airline_sentiment_gold',\n",
    "           'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "X = df['text'].values.tolist()\n",
    "y = df['airline_sentiment'].values.tolist()\n",
    "# Create a dictionary that maps each class name to its corresponding integer value\n",
    "label_map = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "\n",
    "# Convert the labels to their integer representations\n",
    "y = [label_map[i] for i in y]\n",
    "\n",
    "X_train,X_test,y_train,y_test =  train_test_split(X,y,test_size = 0.2)\n",
    "\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Save the model\n",
    "save_directory = f\"pretrained_models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "config = AutoConfig.from_pretrained(save_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7aa3e",
   "metadata": {},
   "source": [
    "#### Text to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d511a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, child in model.named_children():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e56d66bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 1:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 2:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 3:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 4:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 5:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 6:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 7:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 8:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 9:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 10:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 11:\n",
      "  Attention: RobertaAttention(\n",
      "  (self): RobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): RobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  Intermediate: RobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  Output: RobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "    print(f\"Layer {i}:\")\n",
    "    print(f\"  Attention: {layer.attention}\")\n",
    "    print(f\"  Intermediate: {layer.intermediate}\")\n",
    "    print(f\"  Output: {layer.output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71d0ddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_label:3 \n",
      "hidden_size:768 \n"
     ]
    }
   ],
   "source": [
    "num_labels, hidden_size = model.classifier.out_proj.weight.shape\n",
    "print(f'num_label:{num_labels} \\nhidden_size:{hidden_size} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6956eaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) negative 0.7236\n",
      "2) neutral 0.2287\n",
      "3) positive 0.0477\n"
     ]
    }
   ],
   "source": [
    "text = \"Covid cases are increasing fast!\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "# text = \"Covid cases are increasing fast!\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b321b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cpu')\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def Predict(text):\n",
    "    # preprocess the text\n",
    "    text = preprocess(text)\n",
    "\n",
    "    # encode the text\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "    #encoded_input = {name: tensor.to(device) for name, tensor in encoded_input.items()}\n",
    "\n",
    "    # perform sentiment classification\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().cpu().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    # get the predicted sentiment label\n",
    "    ranking = np.argsort(scores)[::-1]\n",
    "    label = config.id2label[ranking[0]]\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "# example usage\n",
    "result = Predict('American Airline is great')\n",
    "print(result) # positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a45932",
   "metadata": {},
   "source": [
    "#### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6125dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CustomClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Apply mean pooling to the hidden states\n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "        # Apply a linear transformation to the pooled hidden states\n",
    "        logits = self.dense(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Replace the classifier with your custom classification head\n",
    "model.classifier = CustomClassificationHead(hidden_size=768, num_labels=3)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa005835",
   "metadata": {},
   "source": [
    "from transformers import RobertaModel, RobertaConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define your new layers\n",
    "new_layers = nn.Sequential(\n",
    "    #nn.Flatten(),\n",
    "    nn.Linear(768,3),\n",
    ")\n",
    "\n",
    "# Replace the classifier with your new layers\n",
    "model.classifier = new_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91155db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze old layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e3bda08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Starting batch 1\n",
      "End batch 1\n",
      "Starting batch 2\n",
      "End batch 2\n",
      "Starting batch 3\n",
      "End batch 3\n",
      "Starting batch 4\n",
      "End batch 4\n",
      "Starting batch 5\n",
      "End batch 5\n",
      "Starting batch 6\n",
      "End batch 6\n",
      "Starting batch 7\n",
      "End batch 7\n",
      "Starting batch 8\n",
      "End batch 8\n",
      "Starting batch 9\n",
      "End batch 9\n",
      "Starting batch 10\n",
      "End batch 10\n",
      "Starting batch 11\n",
      "End batch 11\n",
      "Starting batch 12\n",
      "End batch 12\n",
      "Starting batch 13\n",
      "End batch 13\n",
      "Starting batch 14\n",
      "End batch 14\n",
      "Starting batch 15\n",
      "End batch 15\n",
      "Starting batch 16\n",
      "End batch 16\n",
      "Starting batch 17\n",
      "End batch 17\n",
      "Starting batch 18\n",
      "End batch 18\n",
      "Starting batch 19\n",
      "End batch 19\n",
      "Starting batch 20\n",
      "End batch 20\n",
      "Starting batch 21\n",
      "End batch 21\n",
      "Starting batch 22\n",
      "End batch 22\n",
      "Starting batch 23\n",
      "End batch 23\n",
      "Starting batch 24\n",
      "End batch 24\n",
      "Starting batch 25\n",
      "End batch 25\n",
      "Starting batch 26\n",
      "End batch 26\n",
      "Starting batch 27\n",
      "End batch 27\n",
      "Starting batch 28\n",
      "End batch 28\n",
      "Starting batch 29\n",
      "End batch 29\n",
      "Starting batch 30\n",
      "End batch 30\n",
      "Starting batch 31\n",
      "End batch 31\n",
      "Starting batch 32\n",
      "End batch 32\n",
      "Starting batch 33\n",
      "End batch 33\n",
      "Starting batch 34\n",
      "End batch 34\n",
      "Starting batch 35\n",
      "End batch 35\n",
      "Starting batch 36\n",
      "End batch 36\n",
      "Starting batch 37\n",
      "End batch 37\n",
      "Starting batch 38\n",
      "End batch 38\n",
      "Starting batch 39\n",
      "End batch 39\n",
      "Starting batch 40\n",
      "End batch 40\n",
      "Starting batch 41\n",
      "End batch 41\n",
      "Starting batch 42\n",
      "End batch 42\n",
      "Starting batch 43\n",
      "End batch 43\n",
      "Starting batch 44\n",
      "End batch 44\n",
      "Starting batch 45\n",
      "End batch 45\n",
      "Starting batch 46\n",
      "End batch 46\n",
      "Starting batch 47\n",
      "End batch 47\n",
      "Starting batch 48\n",
      "End batch 48\n",
      "Starting batch 49\n",
      "End batch 49\n",
      "Starting batch 50\n",
      "End batch 50\n",
      "Starting batch 51\n",
      "End batch 51\n",
      "Starting batch 52\n",
      "End batch 52\n",
      "Starting batch 53\n",
      "End batch 53\n",
      "Starting batch 54\n",
      "End batch 54\n",
      "Starting batch 55\n",
      "End batch 55\n",
      "Starting batch 56\n",
      "End batch 56\n",
      "Starting batch 57\n",
      "End batch 57\n",
      "Starting batch 58\n",
      "End batch 58\n",
      "Starting batch 59\n",
      "End batch 59\n",
      "Starting batch 60\n",
      "End batch 60\n",
      "Starting batch 61\n",
      "End batch 61\n",
      "Starting batch 62\n",
      "End batch 62\n",
      "Starting batch 63\n",
      "End batch 63\n",
      "Starting batch 64\n",
      "End batch 64\n",
      "Starting batch 65\n",
      "End batch 65\n",
      "Starting batch 66\n",
      "End batch 66\n",
      "Starting batch 67\n",
      "End batch 67\n",
      "Starting batch 68\n",
      "End batch 68\n",
      "Starting batch 69\n",
      "End batch 69\n",
      "Starting batch 70\n",
      "End batch 70\n",
      "Starting batch 71\n",
      "End batch 71\n",
      "Starting batch 72\n",
      "End batch 72\n",
      "Starting batch 73\n",
      "End batch 73\n",
      "Starting batch 74\n",
      "End batch 74\n",
      "Starting batch 75\n",
      "End batch 75\n",
      "Starting batch 76\n",
      "End batch 76\n",
      "Starting batch 77\n",
      "End batch 77\n",
      "Starting batch 78\n",
      "End batch 78\n",
      "Starting batch 79\n",
      "End batch 79\n",
      "Starting batch 80\n",
      "End batch 80\n",
      "Starting batch 81\n",
      "End batch 81\n",
      "Starting batch 82\n",
      "End batch 82\n",
      "Starting batch 83\n",
      "End batch 83\n",
      "Starting batch 84\n",
      "End batch 84\n",
      "Starting batch 85\n",
      "End batch 85\n",
      "Starting batch 86\n",
      "End batch 86\n",
      "Starting batch 87\n",
      "End batch 87\n",
      "Starting batch 88\n",
      "End batch 88\n",
      "Starting batch 89\n",
      "End batch 89\n",
      "Starting batch 90\n",
      "End batch 90\n",
      "Starting batch 91\n",
      "End batch 91\n",
      "Starting batch 92\n",
      "End batch 92\n",
      "Starting batch 93\n",
      "End batch 93\n",
      "Starting batch 94\n",
      "End batch 94\n",
      "Starting batch 95\n",
      "End batch 95\n",
      "Starting batch 96\n",
      "End batch 96\n",
      "Starting batch 97\n",
      "End batch 97\n",
      "Starting batch 98\n",
      "End batch 98\n",
      "Starting batch 99\n",
      "End batch 99\n",
      "Starting batch 100\n",
      "End batch 100\n",
      "Starting batch 101\n",
      "End batch 101\n",
      "Starting batch 102\n",
      "End batch 102\n",
      "Starting batch 103\n",
      "End batch 103\n",
      "Starting batch 104\n",
      "End batch 104\n",
      "Starting batch 105\n",
      "End batch 105\n",
      "Starting batch 106\n",
      "End batch 106\n",
      "Starting batch 107\n",
      "End batch 107\n",
      "Starting batch 108\n",
      "End batch 108\n",
      "Starting batch 109\n",
      "End batch 109\n",
      "Starting batch 110\n",
      "End batch 110\n",
      "Starting batch 111\n",
      "End batch 111\n",
      "Starting batch 112\n",
      "End batch 112\n",
      "Starting batch 113\n",
      "End batch 113\n",
      "Starting batch 114\n",
      "End batch 114\n",
      "Starting batch 115\n",
      "End batch 115\n",
      "Starting batch 116\n",
      "End batch 116\n",
      "Starting batch 117\n",
      "End batch 117\n",
      "Starting batch 118\n",
      "End batch 118\n",
      "Starting batch 119\n",
      "End batch 119\n",
      "Starting batch 120\n",
      "End batch 120\n",
      "Starting batch 121\n",
      "End batch 121\n",
      "Starting batch 122\n",
      "End batch 122\n",
      "Starting batch 123\n",
      "End batch 123\n",
      "Starting batch 124\n",
      "End batch 124\n",
      "Starting batch 125\n",
      "End batch 125\n",
      "Starting batch 126\n",
      "End batch 126\n",
      "Starting batch 127\n",
      "End batch 127\n",
      "Starting batch 128\n",
      "End batch 128\n",
      "Starting batch 129\n",
      "End batch 129\n",
      "Starting batch 130\n",
      "End batch 130\n",
      "Starting batch 131\n",
      "End batch 131\n",
      "Starting batch 132\n",
      "End batch 132\n",
      "Starting batch 133\n",
      "End batch 133\n",
      "Starting batch 134\n",
      "End batch 134\n",
      "Starting batch 135\n",
      "End batch 135\n",
      "Starting batch 136\n",
      "End batch 136\n",
      "Starting batch 137\n",
      "End batch 137\n",
      "Starting batch 138\n",
      "End batch 138\n",
      "Starting batch 139\n",
      "End batch 139\n",
      "Starting batch 140\n",
      "End batch 140\n",
      "Starting batch 141\n",
      "End batch 141\n",
      "Starting batch 142\n",
      "End batch 142\n",
      "Starting batch 143\n",
      "End batch 143\n",
      "Starting batch 144\n",
      "End batch 144\n",
      "Starting batch 145\n",
      "End batch 145\n",
      "Starting batch 146\n",
      "End batch 146\n",
      "Starting batch 147\n",
      "End batch 147\n",
      "Starting batch 148\n",
      "End batch 148\n",
      "Starting batch 149\n",
      "End batch 149\n",
      "Starting batch 150\n",
      "End batch 150\n",
      "Starting batch 151\n",
      "End batch 151\n",
      "Starting batch 152\n",
      "End batch 152\n",
      "Starting batch 153\n",
      "End batch 153\n",
      "Starting batch 154\n",
      "End batch 154\n",
      "Starting batch 155\n",
      "End batch 155\n",
      "Starting batch 156\n",
      "End batch 156\n",
      "Starting batch 157\n",
      "End batch 157\n",
      "Starting batch 158\n",
      "End batch 158\n",
      "Starting batch 159\n",
      "End batch 159\n",
      "Starting batch 160\n",
      "End batch 160\n",
      "Starting batch 161\n",
      "End batch 161\n",
      "Starting batch 162\n",
      "End batch 162\n",
      "Starting batch 163\n",
      "End batch 163\n",
      "Starting batch 164\n",
      "End batch 164\n",
      "Starting batch 165\n",
      "End batch 165\n",
      "Starting batch 166\n",
      "End batch 166\n",
      "Starting batch 167\n",
      "End batch 167\n",
      "Starting batch 168\n",
      "End batch 168\n",
      "Starting batch 169\n",
      "End batch 169\n",
      "Starting batch 170\n",
      "End batch 170\n",
      "Starting batch 171\n",
      "End batch 171\n",
      "Starting batch 172\n",
      "End batch 172\n",
      "Starting batch 173\n",
      "End batch 173\n",
      "Starting batch 174\n",
      "End batch 174\n",
      "Starting batch 175\n",
      "End batch 175\n",
      "Starting batch 176\n",
      "End batch 176\n",
      "Starting batch 177\n",
      "End batch 177\n",
      "Starting batch 178\n",
      "End batch 178\n",
      "Starting batch 179\n",
      "End batch 179\n",
      "Starting batch 180\n",
      "End batch 180\n",
      "Starting batch 181\n",
      "End batch 181\n",
      "Starting batch 182\n",
      "End batch 182\n",
      "Starting batch 183\n",
      "End batch 183\n",
      "Starting batch 184\n",
      "End batch 184\n",
      "Starting batch 185\n",
      "End batch 185\n",
      "Starting batch 186\n",
      "End batch 186\n",
      "Starting batch 187\n",
      "End batch 187\n",
      "Starting batch 188\n",
      "End batch 188\n",
      "Starting batch 189\n",
      "End batch 189\n",
      "Starting batch 190\n",
      "End batch 190\n",
      "Starting batch 191\n",
      "End batch 191\n",
      "Starting batch 192\n",
      "End batch 192\n",
      "Starting batch 193\n",
      "End batch 193\n",
      "Starting batch 194\n",
      "End batch 194\n",
      "Starting batch 195\n",
      "End batch 195\n",
      "Starting batch 196\n",
      "End batch 196\n",
      "Starting batch 197\n",
      "End batch 197\n",
      "Starting batch 198\n",
      "End batch 198\n",
      "Starting batch 199\n",
      "End batch 199\n",
      "Starting batch 200\n",
      "End batch 200\n",
      "Starting batch 201\n",
      "End batch 201\n",
      "Starting batch 202\n",
      "End batch 202\n",
      "Starting batch 203\n",
      "End batch 203\n",
      "Starting batch 204\n",
      "End batch 204\n",
      "Starting batch 205\n",
      "End batch 205\n",
      "Starting batch 206\n",
      "End batch 206\n",
      "Starting batch 207\n",
      "End batch 207\n",
      "Starting batch 208\n",
      "End batch 208\n",
      "Starting batch 209\n",
      "End batch 209\n",
      "Starting batch 210\n",
      "End batch 210\n",
      "Starting batch 211\n",
      "End batch 211\n",
      "Starting batch 212\n",
      "End batch 212\n",
      "Starting batch 213\n",
      "End batch 213\n",
      "Starting batch 214\n",
      "End batch 214\n",
      "Starting batch 215\n",
      "End batch 215\n",
      "Starting batch 216\n",
      "End batch 216\n",
      "Starting batch 217\n",
      "End batch 217\n",
      "Starting batch 218\n",
      "End batch 218\n",
      "Starting batch 219\n",
      "End batch 219\n",
      "Starting batch 220\n",
      "End batch 220\n",
      "Starting batch 221\n",
      "End batch 221\n",
      "Starting batch 222\n",
      "End batch 222\n",
      "Starting batch 223\n",
      "End batch 223\n",
      "Starting batch 224\n",
      "End batch 224\n",
      "Starting batch 225\n",
      "End batch 225\n",
      "Starting batch 226\n",
      "End batch 226\n",
      "Starting batch 227\n",
      "End batch 227\n",
      "Starting batch 228\n",
      "End batch 228\n",
      "Starting batch 229\n",
      "End batch 229\n",
      "Starting batch 230\n",
      "End batch 230\n",
      "Starting batch 231\n",
      "End batch 231\n",
      "Starting batch 232\n",
      "End batch 232\n",
      "Starting batch 233\n",
      "End batch 233\n",
      "Starting batch 234\n",
      "End batch 234\n",
      "Starting batch 235\n",
      "End batch 235\n",
      "Starting batch 236\n",
      "End batch 236\n",
      "Starting batch 237\n",
      "End batch 237\n",
      "Starting batch 238\n",
      "End batch 238\n",
      "Starting batch 239\n",
      "End batch 239\n",
      "Starting batch 240\n",
      "End batch 240\n",
      "Starting batch 241\n",
      "End batch 241\n",
      "Starting batch 242\n",
      "End batch 242\n",
      "Starting batch 243\n",
      "End batch 243\n",
      "Starting batch 244\n",
      "End batch 244\n",
      "Starting batch 245\n",
      "End batch 245\n",
      "Starting batch 246\n",
      "End batch 246\n",
      "Starting batch 247\n",
      "End batch 247\n",
      "Starting batch 248\n",
      "End batch 248\n",
      "Starting batch 249\n",
      "End batch 249\n",
      "Starting batch 250\n",
      "End batch 250\n",
      "Starting batch 251\n",
      "End batch 251\n",
      "Starting batch 252\n",
      "End batch 252\n",
      "Starting batch 253\n",
      "End batch 253\n",
      "Starting batch 254\n",
      "End batch 254\n",
      "Starting batch 255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End batch 255\n",
      "Starting batch 256\n",
      "End batch 256\n",
      "Starting batch 257\n",
      "End batch 257\n",
      "Starting batch 258\n",
      "End batch 258\n",
      "Starting batch 259\n",
      "End batch 259\n",
      "Starting batch 260\n",
      "End batch 260\n",
      "Starting batch 261\n",
      "End batch 261\n",
      "Starting batch 262\n",
      "End batch 262\n",
      "Starting batch 263\n",
      "End batch 263\n",
      "Starting batch 264\n",
      "End batch 264\n",
      "Starting batch 265\n",
      "End batch 265\n",
      "Starting batch 266\n",
      "End batch 266\n",
      "Starting batch 267\n",
      "End batch 267\n",
      "Starting batch 268\n",
      "End batch 268\n",
      "Starting batch 269\n",
      "End batch 269\n",
      "Starting batch 270\n",
      "End batch 270\n",
      "Starting batch 271\n",
      "End batch 271\n",
      "Starting batch 272\n",
      "End batch 272\n",
      "Starting batch 273\n",
      "End batch 273\n",
      "Starting batch 274\n",
      "End batch 274\n",
      "Starting batch 275\n",
      "End batch 275\n",
      "Starting batch 276\n",
      "End batch 276\n",
      "Starting batch 277\n",
      "End batch 277\n",
      "Starting batch 278\n",
      "End batch 278\n",
      "Starting batch 279\n",
      "End batch 279\n",
      "Starting batch 280\n",
      "End batch 280\n",
      "Starting batch 281\n",
      "End batch 281\n",
      "Starting batch 282\n",
      "End batch 282\n",
      "Starting batch 283\n",
      "End batch 283\n",
      "Starting batch 284\n",
      "End batch 284\n",
      "Starting batch 285\n",
      "End batch 285\n",
      "Starting batch 286\n",
      "End batch 286\n",
      "Starting batch 287\n",
      "End batch 287\n",
      "Starting batch 288\n",
      "End batch 288\n",
      "Starting batch 289\n",
      "End batch 289\n",
      "Starting batch 290\n",
      "End batch 290\n",
      "Starting batch 291\n",
      "End batch 291\n",
      "Starting batch 292\n",
      "End batch 292\n",
      "Starting batch 293\n",
      "End batch 293\n",
      "Starting batch 294\n",
      "End batch 294\n",
      "Starting batch 295\n",
      "End batch 295\n",
      "Starting batch 296\n",
      "End batch 296\n",
      "Starting batch 297\n",
      "End batch 297\n",
      "Starting batch 298\n",
      "End batch 298\n",
      "Starting batch 299\n",
      "End batch 299\n",
      "Starting batch 300\n",
      "End batch 300\n",
      "Starting batch 301\n",
      "End batch 301\n",
      "Starting batch 302\n",
      "End batch 302\n",
      "Starting batch 303\n",
      "End batch 303\n",
      "Starting batch 304\n",
      "End batch 304\n",
      "Starting batch 305\n",
      "End batch 305\n",
      "Starting batch 306\n",
      "End batch 306\n",
      "Starting batch 307\n",
      "End batch 307\n",
      "Starting batch 308\n",
      "End batch 308\n",
      "Starting batch 309\n",
      "End batch 309\n",
      "Starting batch 310\n",
      "End batch 310\n",
      "Starting batch 311\n",
      "End batch 311\n",
      "Starting batch 312\n",
      "End batch 312\n",
      "Starting batch 313\n",
      "End batch 313\n",
      "Starting batch 314\n",
      "End batch 314\n",
      "Starting batch 315\n",
      "End batch 315\n",
      "Starting batch 316\n",
      "End batch 316\n",
      "Starting batch 317\n",
      "End batch 317\n",
      "Starting batch 318\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 │   │   </span>attention_mask = batch[<span style=\"color: #808000; text-decoration-color: #808000\">'attention_mask'</span>].to(device)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">48 │   │   </span>labels = batch[<span style=\"color: #808000; text-decoration-color: #808000\">'labels'</span>].to(device)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>50 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=label    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 │   │   </span>loss = outputs.loss                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 │   │   </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">53 │   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">g_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1211</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1208 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1209 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1210 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1211 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.roberta(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1212 │   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1213 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1214 │   │   │   </span>token_type_ids=token_type_ids,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">g_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">851</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 848 │   │   │   </span>inputs_embeds=inputs_embeds,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 849 │   │   │   </span>past_key_values_length=past_key_values_length,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 850 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 851 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>encoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 852 │   │   │   </span>embedding_output,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 853 │   │   │   </span>attention_mask=extended_attention_mask,                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 854 │   │   │   </span>head_mask=head_mask,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">g_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">526</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 523 │   │   │   │   │   </span>encoder_attention_mask,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 524 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 525 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 526 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>layer_outputs = layer_module(                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 527 │   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 528 │   │   │   │   │   </span>attention_mask,                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 529 │   │   │   │   │   </span>layer_head_mask,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">g_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">453</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 450 │   │   │   </span>cross_attn_present_key_value = cross_attention_outputs[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 451 │   │   │   </span>present_key_value = present_key_value + cross_attn_present_key_value          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 452 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 453 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>layer_output = apply_chunking_to_forward(                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 454 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.feed_forward_chunk, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.chunk_size_feed_forward, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.seq_len_dim, att  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 455 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 456 │   │   </span>outputs = (layer_output,) + outputs                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\pytorch_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">249</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_chunking_to_forward</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">246 │   │   # concatenate output at same dimension</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">247 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.cat(output_chunks, dim=chunk_dim)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">248 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>249 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_fn(*input_tensors)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">250 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">251 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">252 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">find_pruneable_heads_and_indices</span>(                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">g_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">465</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">feed_forward_chunk</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 462 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> outputs                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 463 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 464 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">feed_forward_chunk</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, attention_output):                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 465 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>intermediate_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.intermediate(attention_output)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 466 │   │   </span>layer_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.output(intermediate_output, attention_output)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 467 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> layer_output                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 468 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">g_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">363</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 360 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.intermediate_act_fn = config.hidden_act                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 361 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 362 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hidden_states: torch.Tensor) -&gt; torch.Tensor:                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 363 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dense(hidden_states)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 364 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.intermediate_act_fn(hidden_states)                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 365 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> hidden_states                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 366 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   │   </span>init.uniform_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, -bound, bound)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor) -&gt; Tensor:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">extra_repr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #808000; text-decoration-color: #808000\">'in_features={}, out_features={}, bias={}'</span>.format(                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m47 \u001b[0m\u001b[2m│   │   \u001b[0mattention_mask = batch[\u001b[33m'\u001b[0m\u001b[33mattention_mask\u001b[0m\u001b[33m'\u001b[0m].to(device)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m48 \u001b[0m\u001b[2m│   │   \u001b[0mlabels = batch[\u001b[33m'\u001b[0m\u001b[33mlabels\u001b[0m\u001b[33m'\u001b[0m].to(device)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m49 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m50 \u001b[2m│   │   \u001b[0moutputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=label    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m51 \u001b[0m\u001b[2m│   │   \u001b[0mloss = outputs.loss                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m│   │   \u001b[0mloss.backward()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m53 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mg_roberta.py\u001b[0m:\u001b[94m1211\u001b[0m in \u001b[92mforward\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1208 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1209 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1210 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1211 \u001b[2m│   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.roberta(                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1212 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1213 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1214 \u001b[0m\u001b[2m│   │   │   \u001b[0mtoken_type_ids=token_type_ids,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mg_roberta.py\u001b[0m:\u001b[94m851\u001b[0m in \u001b[92mforward\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 848 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs_embeds=inputs_embeds,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 849 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values_length=past_key_values_length,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 850 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 851 \u001b[2m│   │   \u001b[0mencoder_outputs = \u001b[96mself\u001b[0m.encoder(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 852 \u001b[0m\u001b[2m│   │   │   \u001b[0membedding_output,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 853 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=extended_attention_mask,                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 854 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask=head_mask,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mg_roberta.py\u001b[0m:\u001b[94m526\u001b[0m in \u001b[92mforward\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 523 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mencoder_attention_mask,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 524 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 525 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 526 \u001b[2m│   │   │   │   \u001b[0mlayer_outputs = layer_module(                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 527 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 528 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask,                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 529 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlayer_head_mask,                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mg_roberta.py\u001b[0m:\u001b[94m453\u001b[0m in \u001b[92mforward\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 450 \u001b[0m\u001b[2m│   │   │   \u001b[0mcross_attn_present_key_value = cross_attention_outputs[-\u001b[94m1\u001b[0m]                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 451 \u001b[0m\u001b[2m│   │   │   \u001b[0mpresent_key_value = present_key_value + cross_attn_present_key_value          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 452 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 453 \u001b[2m│   │   \u001b[0mlayer_output = apply_chunking_to_forward(                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 454 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.feed_forward_chunk, \u001b[96mself\u001b[0m.chunk_size_feed_forward, \u001b[96mself\u001b[0m.seq_len_dim, att  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 455 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 456 \u001b[0m\u001b[2m│   │   \u001b[0moutputs = (layer_output,) + outputs                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\pytorch_utils.py\u001b[0m:\u001b[94m249\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mapply_chunking_to_forward\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m246 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# concatenate output at same dimension\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m247 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.cat(output_chunks, dim=chunk_dim)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m248 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m249 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m forward_fn(*input_tensors)                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m250 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m251 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m252 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfind_pruneable_heads_and_indices\u001b[0m(                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mg_roberta.py\u001b[0m:\u001b[94m465\u001b[0m in \u001b[92mfeed_forward_chunk\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 462 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m outputs                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 463 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 464 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfeed_forward_chunk\u001b[0m(\u001b[96mself\u001b[0m, attention_output):                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 465 \u001b[2m│   │   \u001b[0mintermediate_output = \u001b[96mself\u001b[0m.intermediate(attention_output)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 466 \u001b[0m\u001b[2m│   │   \u001b[0mlayer_output = \u001b[96mself\u001b[0m.output(intermediate_output, attention_output)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 467 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m layer_output                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 468 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\roberta\\modelin\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mg_roberta.py\u001b[0m:\u001b[94m363\u001b[0m in \u001b[92mforward\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 360 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.intermediate_act_fn = config.hidden_act                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 361 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 362 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, hidden_states: torch.Tensor) -> torch.Tensor:                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 363 \u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.dense(hidden_states)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 364 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.intermediate_act_fn(hidden_states)                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 365 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m hidden_states                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 366 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\formy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92mforward\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   \u001b[0minit.uniform_(\u001b[96mself\u001b[0m.bias, -bound, bound)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor) -> Tensor:                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m F.linear(\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.bias)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mextra_repr\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mstr\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[33m'\u001b[0m\u001b[33min_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, out_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, bias=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m.format(                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a dataset\n",
    "class AirlineSentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = preprocess(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "    # Initialize your dataset and dataloader\n",
    "dataset = AirlineSentimentDataset(X_train, y_train, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # You can change the batch_size depending on your GPU memory\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "device = 'cuda'\n",
    "# Training loop\n",
    "for epoch in range(1):  # You can change the number of epochs\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        #print(f'Starting batch {i+1}')\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        #print(f'End batch {i+1}')\n",
    "    print(f'Epoch {epoch+1}/{100} | Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)\n",
    "batch = next(dataiter)\n",
    "inputs = batch['input_ids']\n",
    "attention_mask = batch['attention_mask']\n",
    "labels = batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64290435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.shape)\n",
    "print(attention_mask.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "624634e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "text = preprocess('American Airlines is super great')\n",
    "# encode the text\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "#encoded_input = {name: tensor.to(device) for name, tensor in encoded_input.items()}\n",
    "# perform sentiment classification\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().cpu().numpy()\n",
    "scores = softmax(scores)\n",
    "# get the predicted sentiment label\n",
    "ranking = np.argsort(scores)[::-1]\n",
    "label = config.id2label[ranking[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "66049b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2980683 , 0.40511227, 0.29681948], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6421780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87359e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0258fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
